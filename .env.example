# =============================================================================
# RAG Platform Configuration
# =============================================================================
# Copy this file to .env and fill in your values
# All API keys are OPTIONAL - the platform works in local-first mode without them

# =============================================================================
# API KEYS (Optional - for cloud-enhanced mode)
# =============================================================================

# OpenAI API Key - for embeddings and LLM generation
# Get yours at: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-...

# Anthropic API Key - for Claude LLM fallback
# Get yours at: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-...

# =============================================================================
# QDRANT VECTOR DATABASE
# =============================================================================

# Qdrant connection URL (default: local Docker instance)
QDRANT_URL=http://localhost:6333

# Collection name for storing chunks
QDRANT_COLLECTION=rag_chunks

# Optional: Qdrant API key (for Qdrant Cloud)
# QDRANT_API_KEY=

# =============================================================================
# EMBEDDING CONFIGURATION
# =============================================================================

# Embedding provider: openai | local
# - openai: Uses OpenAI text-embedding-3-small (requires OPENAI_API_KEY)
# - local: Uses sentence-transformers (no API key needed)
EMBEDDING_PROVIDER=openai

# OpenAI embedding model (when EMBEDDING_PROVIDER=openai)
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Local embedding model (when EMBEDDING_PROVIDER=local)
# Options: all-MiniLM-L6-v2, all-mpnet-base-v2, BAAI/bge-small-en-v1.5
LOCAL_EMBEDDING_MODEL=all-MiniLM-L6-v2

# =============================================================================
# LLM CONFIGURATION
# =============================================================================

# Primary LLM provider: openai | anthropic | ollama
LLM_PROVIDER=openai

# OpenAI model (when LLM_PROVIDER=openai)
OPENAI_MODEL=gpt-4o-mini

# Anthropic model (when LLM_PROVIDER=anthropic)
ANTHROPIC_MODEL=claude-3-haiku-20240307

# Enable automatic fallback to secondary provider
LLM_FALLBACK_ENABLED=true

# Ollama configuration (for local-first mode)
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# =============================================================================
# RETRIEVAL CONFIGURATION
# =============================================================================

# Reciprocal Rank Fusion constant (higher = less weight on top ranks)
RRF_K=60

# Freshness scoring
FRESHNESS_HALF_LIFE_DAYS=30
FRESHNESS_WEIGHT=0.2

# Default retrieval limits
RETRIEVAL_INITIAL_LIMIT=100
RETRIEVAL_RERANK_LIMIT=20
RETRIEVAL_FINAL_LIMIT=5

# =============================================================================
# CHUNKING CONFIGURATION
# =============================================================================

# Default chunk size in tokens
CHUNK_SIZE=512

# Chunk overlap in tokens
CHUNK_OVERLAP=50

# Default chunking strategy: recursive | heading | sliding_window | semantic
DEFAULT_CHUNKING_STRATEGY=recursive

# =============================================================================
# API CONFIGURATION
# =============================================================================

# API host and port
API_HOST=0.0.0.0
API_PORT=8000

# Enable debug mode (more verbose logging)
DEBUG=false

# Log level: DEBUG | INFO | WARNING | ERROR
LOG_LEVEL=INFO

# Maximum file upload size in bytes (default: 50MB)
MAX_UPLOAD_SIZE=52428800

# =============================================================================
# BM25 INDEX PERSISTENCE
# =============================================================================

# Path to store BM25 index (relative to project root)
BM25_INDEX_PATH=data/bm25_index.pkl

# =============================================================================
# RERANKER CONFIGURATION
# =============================================================================

# Cross-encoder model for reranking
RERANKER_MODEL=BAAI/bge-reranker-base

# Batch size for reranking inference
RERANKER_BATCH_SIZE=32
